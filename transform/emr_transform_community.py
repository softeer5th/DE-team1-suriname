from functools import partial
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
# import conf
import urllib.request
import json
import argparse
import ast
import base64
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed # ë©€í‹°ìŠ¤ë ˆë”©
from collections import defaultdict
from pyspark.sql import Row

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def transform(data_source:str, output_uri:str, batch_period:str, community_accident_keyword:str, gpt, issue_list)-> None:
    with (
        SparkSession.builder.appName(f"transform news at {batch_period}").config("spark.driver.bindAddress", "0.0.0.0")
                .config("spark.sql.session.timeZone", "UTC")
                # ë¡œì»¬ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œí‚¬ë•Œ config ì ìš©
                # .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
                # .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")
                .getOrCreate()as spark
    ):
        
        data_schema = StructType([
            StructField('post_time', TimestampType(), True),
            StructField('title', StringType(), True),
            StructField('content', StringType(), True),
            StructField('view_count', LongType(), True),
            StructField('like_count', LongType(), True),
            StructField('source', StringType(), True),
            StructField('link', StringType(), True)
        ])

        # base64 ë””ì½”ë”©
        issue_list = json.loads(base64.b64decode(issue_list).decode('utf-8'))
        community_accident_keyword = json.loads(base64.b64decode(community_accident_keyword).decode('utf-8'))

        issue_list = ast.literal_eval(issue_list)
        community_accident_keyword = ast.literal_eval(community_accident_keyword)
        logger.info(f"formatted issue_list: {issue_list}")
        logger.info(f"formatted community_accident_keyword: {community_accident_keyword}")


        df = spark.read.schema(data_schema).parquet(data_source + batch_period + '/')

        
        # issue_listì— í•´ë‹¹í•˜ëŠ” ì‚¬ê³  ìœ í˜•ë§Œ ì¶”ì¶œ
        selected_keywords = {issue['accident']: community_accident_keyword[issue['accident']] for issue in issue_list if issue['accident'] in community_accident_keyword}
        logger.info(f"selected_keywords: {selected_keywords}")

        # ğŸ”¹ `car_model` ë§¤ì¹­ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„± (accident â†’ car_model ë§¤í•‘)
        accident_to_car_model = defaultdict(set)
        for issue in issue_list:
            accident_to_car_model[issue['accident']].add(issue['car_model'])
        
        # setì„ listë¡œ ë³€í™˜
        accident_to_car_model = {k: list(v) for k, v in accident_to_car_model.items()}

        logger.info(f"accident_to_car_model: {accident_to_car_model}")

        # ì´ë¶€ë¶„ ë°”ë€Œì–´ì•¼ í•¨.
        df = df.withColumn(
            'accident',
            F.array_distinct(F.array(*[
                F.when(F.col('title').contains(subkey) | F.col('content').contains(subkey), accident)
                for accident, subkeys in selected_keywords.items()
                for subkey in subkeys
            ]))
        )
        df.show()

        # TEST: ëª¨ë“  í–‰ì„ ê¸‰ë°œì§„ ì‚¬ê±´ìœ¼ë¡œ ì„¤ì •
        # df = df.withColumn(
        #     'accident',
        #     F.array(F.lit("ê¸‰ë°œì§„"))
        # )
        df_exploded = df.withColumn("accident", F.explode(F.col("accident"))) \
            .filter(F.col("accident").isNotNull())
        
        # ğŸš€ `accident_to_car_model`ì„ Spark DataFrameìœ¼ë¡œ ë³€í™˜
        mapping_data = [(accident, car_model) for accident, car_models in accident_to_car_model.items() for car_model in car_models]
        mapping_df = spark.createDataFrame([Row(accident=a, car_model=c) for a, c in mapping_data])

        # ğŸš€ `accident` ê¸°ì¤€ìœ¼ë¡œ `join`
        df_exploded = df_exploded.join(mapping_df, on="accident", how="left")

        # ğŸ”¹ `car_model`ì´ issue_listì— ìˆëŠ” ìë™ì°¨ ëª¨ë¸ë§Œ í•„í„°ë§
        # car_models = [issue["car_model"] for issue in issue_list]
        # df_filtered = df_exploded.filter(F.col("car_model").isin(car_models))
        

        score_schema = df_exploded.schema.add('comm_score', IntegerType(), True)
        gpt = json.loads(gpt)
        score_rdd = df_exploded.rdd.mapPartitions(partial(score_rdd_generator, param = gpt))
        scored_df = score_rdd.toDF(score_schema).cache()
        scored_df.show()

        grouped_df = scored_df.groupBy("car_model", "accident").agg(
            F.count("*").alias("count"),
            F.to_json(
                F.collect_list(
                    F.struct("content", "link")
                )
            ).alias("comm"),
            F.avg("comm_score").alias("avg_comm_score")
        )
        grouped_df.show()

        grouped_df.coalesce(1).write.mode('overwrite').parquet(output_uri + batch_period + '/')
    return

# ìˆ˜ì • ì „ í•¨ìˆ˜
# def score_rdd_generator(partition, param):
#     api_key = param["api_key"]
#     model = param["model"]
#     for row in partition:
#         row_with_score = row.asDict().copy()
#         try:
#             row_with_score['comm_score'] = get_score_from_gpt(
#                 car_model=row_with_score['car_model'],
#                 accident=row_with_score['accident'],
#                 title = row_with_score['title'],
#                 content=row_with_score['content'],
#                 api_key=api_key,
#                 model=model
#             )
#             yield row_with_score
#         except Exception as e:
#             print(f"Error processing row: {e}")
#             row_with_score['comm_score'] = 0
#             yield row_with_score

# ìˆ˜ì • í›„ í•¨ìˆ˜
def score_rdd_generator(partition, param):
    api_key = param["api_key"]
    model = param["model"]

    partition_list = list(partition)  # ğŸ”¥ partitionì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë©€í‹°ì“°ë ˆë”© ì ìš© ê°€ëŠ¥)

    if not partition_list:
        return iter([])  # ë¹ˆ partition ì²˜ë¦¬

    # âœ… ğŸ”¥ ë³‘ë ¬ API í˜¸ì¶œ ì ìš© (ë©€í‹°ì“°ë ˆë”©)
    results = get_scores_in_parallel(partition_list, api_key, model, max_threads=10)

    return iter(results)  # Spark RDDëŠ” ì´í„°ë ˆì´í„° ë°˜í™˜ í•„ìš”


def get_score_from_gpt(car_model:str, accident:str, title:str, content: str, api_key: str, model: str = "gpt-4o-mini") -> int:
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": f"""
                [ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ ìš”ì²­]

                [í•  ê²ƒ]
                ë„ˆëŠ” ìë™ì°¨ ê´€ë ¨ ê¸€ì—ì„œ íŠ¹ì • ì°¨ëŸ‰ê³¼ ì‚¬ê³  ìœ í˜•ì— ëŒ€í•œ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” AIì•¼.
                ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì°¨ëŸ‰ ëª¨ë¸ê³¼ ì‚¬ê³  ìœ í˜•**(ì˜ˆ: '{car_model}' + '{accident}')ì— ëŒ€í•œ ì „ì²´ì ì¸ ê°ì„±ì„ ë¶„ì„í•´ì¤˜.

                ê° í–‰ì—ëŠ” ì œëª©(title), ë³¸ë¬¸(content)ì´ í¬í•¨ë˜ì–´ ìˆì–´.
                ì´ ëª¨ë“  ìš”ì†Œë¥¼ ì¢…í•©í•˜ì—¬ **í•´ë‹¹ ì°¨ëŸ‰ê³¼ ì‚¬ê³  ìœ í˜•ì— ëŒ€í•œ ê°ì„± ì ìˆ˜**ë¥¼ 100(ë§¤ìš° ë¶€ì •ì )ì—ì„œ 0(ë§¤ìš° ê¸ì •ì )ê¹Œì§€ì˜ ë²”ìœ„ë¡œ ìˆ«ìë¡œ í‰ê°€í•´ì¤˜.
                50ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¤‘ë¦½ì ì´ë©°, 100ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶€ì •ì , 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê¸ì •ì ì¸ê±°ì•¼.
                ë°˜í™˜ì€ ë°˜ë“œì‹œ 0~100ì‚¬ì´ì˜ ì •ìˆ˜ ìˆ«ìë§Œ ë°˜í™˜í•˜ê³  ì•„ë¬´ëŸ° ë§ë„ í•˜ì§€ë§ˆ.
                ì•„ë˜ëŠ” ë¶„ì„í•  ë°ì´í„°ì•¼:

                ---
                **ì°¨ëŸ‰ ëª¨ë¸:** {car_model}  
                **ì‚¬ê³  ìœ í˜•:** {accident}  
                **ì œëª©:** {title}  
                **ë³¸ë¬¸:** {content}
                
                ---
                """
            }
        ],
        "temperature": 0.2
    }
    try:
        req = urllib.request.Request(
            url, 
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )
        with urllib.request.urlopen(req) as response:
            data = json.loads(response.read().decode())

        content = data["choices"][0]["message"]["content"].strip()
        score = int(content)
        print(score)
        return score
    except Exception as e:
        print(f"Error while calling API: {e}. score set to 0.")
        return 0

def get_scores_in_parallel(rows, api_key, model, max_threads=10):
    """ì—¬ëŸ¬ í–‰ì„ ë³‘ë ¬ë¡œ API ìš”ì²­í•˜ì—¬ ê°ì„± ì ìˆ˜ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    results = []

    # ğŸš€ ThreadPoolExecutor ì‚¬ìš© (ìµœëŒ€ max_threadsê°œì˜ API ìš”ì²­ì„ ë™ì‹œì— ì‹¤í–‰)
    with ThreadPoolExecutor(max_threads) as executor:
        future_to_row = {
            executor.submit(get_score_from_gpt, row["car_model"], row["accident"], row["title"], row["content"], api_key, model): row
            for row in rows
        }

        # ì™„ë£Œëœ ìš”ì²­ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ê²°ê³¼ ì²˜ë¦¬
        for future in as_completed(future_to_row):
            row = future_to_row[future]
            row_dict = row.asDict().copy()  # ğŸ”¥ Row ê°ì²´ë¥¼ dictë¡œ ë³€í™˜ í›„ ìˆ˜ì • ê°€ëŠ¥
            try:
                score = future.result()
                row_dict["comm_score"] = score
            except Exception as e:
                print(f"Error processing row: {e}")
                row_dict["comm_score"] = 0  # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’

            results.append(Row(**row_dict))  # ğŸ”¥ dictë¥¼ ë‹¤ì‹œ Row ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€

    return results  # ëª¨ë“  í–‰ì˜ ê°ì„± ì ìˆ˜ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

if __name__ == "__main__":

    # ë¡œì»¬ì—ì„œ ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ
    # data_source = conf.S3_COMMUNITY_DATA
    # output_uri = conf.S3_COMMUNITY_OUTPUT
    # batch_period = conf.S3_COMMUNITY_BATCH_PERIOD
    # transform(data_source, output_uri, batch_period)

    # EMRì—ì„œ ì‹¤í–‰í•  ë•Œ ì£¼ì„ í•´ì œ
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_source", help="s3 data uri")
    parser.add_argument("--output_uri", help="s3 output uri")
    parser.add_argument("--batch_period", help="batch period")
    parser.add_argument("--community_accident_keyword", help="community accident keyword")
    parser.add_argument("--gpt", help="gpt information")
    parser.add_argument("--issue_list", help="issue list")
    args = parser.parse_args()
    
    transform(args.data_source, args.output_uri, args.batch_period, args.community_accident_keyword, args.gpt, args.issue_list)
